"""
UNIFIED QUERY GENERATOR - Generates ALL query types for performance testing.
Handles: BM25, Hybrid (0.1 & 0.9), Vector, and Mixed queries.
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

import json
import argparse
from openai_client import create_sync_openai_client
import config

# Collections for multi-collection testing
MULTI_COLLECTIONS = [
    'SongLyrics', 'SongLyrics_400k', 'SongLyrics_200k',
    'SongLyrics_50k', 'SongLyrics_30k', 'SongLyrics_20k', 'SongLyrics_15k', 'SongLyrics_12k', 'SongLyrics_10k'
]

# Test queries
SEARCH_QUERIES = [
    "love and heartbreak", "summer party vibes", "feeling alone tonight",
    "dance all night long", "broken dreams and hope", "city lights at midnight",
    "memories of yesterday", "living life to fullest", "chasing dreams forever",
    "never give up fighting", "friendship and loyalty", "money power respect",
    "family comes first always", "trust nobody believe yourself", "hustle grind every day",
    "success and motivation", "romantic love story", "pain and suffering",
    "celebration and joy", "freedom and liberty", "hope for better tomorrow",
    "struggle and perseverance", "peace and tranquility", "anger and revenge",
    "happiness and laughter", "sadness and tears", "victory and triumph",
    "loss and defeat", "passion and desire", "fear and courage"
]


def get_embeddings(cache_file='embeddings_cache.json'):
    """Get embeddings for all queries (cached for reuse)"""
    
    # Try to load from cache first
    if os.path.exists(cache_file):
        try:
            print(f"\nüì¶ Loading cached embeddings from {cache_file}...")
            with open(cache_file, 'r') as f:
                embeddings = json.load(f)
            
            # Verify all queries have embeddings
            if all(query in embeddings for query in SEARCH_QUERIES):
                print(f"‚úÖ Loaded {len(embeddings)} cached embeddings")
                return embeddings
            else:
                print("‚ö†Ô∏è  Cache incomplete, regenerating...")
        except Exception as e:
            print(f"‚ö†Ô∏è  Cache error: {e}, regenerating...")
    
    # Generate fresh embeddings
    print("\nüîÑ Generating embeddings for 30 queries (this takes ~30 seconds)...")
    print("   These will be cached for future use!")
    
    client, model = create_sync_openai_client()
    embeddings = {}
    
    for i, query in enumerate(SEARCH_QUERIES, 1):
        print(f"  [{i}/30] {query}...", end=' ', flush=True)
        try:
            response = client.embeddings.create(model=model, input=query)
            embeddings[query] = response.data[0].embedding
            print("‚úì")
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return None
    
    # Save to cache
    try:
        with open(cache_file, 'w') as f:
            json.dump(embeddings, f)
        print(f"\nüíæ Saved embeddings to {cache_file} (will reuse next time)")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Could not save cache: {e}")
    
    return embeddings


def generate_bm25_query(query_text, collections, limit):
    """Generate BM25 query for multiple collections"""
    collection_queries = []
    for collection in collections:
        collection_queries.append(f'''
        {collection}(
          bm25: {{query: "{query_text}", properties: ["title", "lyrics"]}}
          limit: {limit}
        ) {{
          title tag artist year views features lyrics song_id language_cld3 language_ft language
          _additional {{ score }}
        }}''')
    
    all_collections = "\n        ".join(collection_queries)
    return f"{{ Get {{ {all_collections} }} }}"


def generate_hybrid_query(query_text, query_vector, alpha, collections, limit):
    """Generate Hybrid query"""
    vector_str = json.dumps(query_vector)
    collection_queries = []
    
    for collection in collections:
        collection_queries.append(f'''
        {collection}(
          hybrid: {{
            query: "{query_text}"
            alpha: {alpha}
            vector: {vector_str}
            properties: ["title", "lyrics"]
          }}
          limit: {limit}
        ) {{
          title tag artist year views features lyrics song_id language_cld3 language_ft language
          _additional {{ score }}
        }}''')
    
    all_collections = "\n        ".join(collection_queries)
    return f"{{ Get {{ {all_collections} }} }}"


def generate_vector_query(query_vector, collections, limit):
    """Generate pure vector query"""
    vector_str = json.dumps(query_vector)
    collection_queries = []
    
    for collection in collections:
        collection_queries.append(f'''
        {collection}(
          nearVector: {{vector: {vector_str}}}
          limit: {limit}
        ) {{
          title tag artist year views features lyrics song_id language_cld3 language_ft language
          _additional {{ distance certainty }}
        }}''')
    
    all_collections = "\n        ".join(collection_queries)
    return f"{{ Get {{ {all_collections} }} }}"


def generate_all_query_files(test_type, limit, collections, output_dir='.'):
    """Generate query files for specific test type and limit"""
    
    print(f"\n{'='*70}")
    print(f"Generating {test_type.upper()} queries (limit={limit})")
    print(f"{'='*70}")
    
    # Get embeddings if needed
    embeddings = None
    if test_type in ['hybrid_01', 'hybrid_09', 'vector', 'mixed']:
        embeddings = get_embeddings()
        if not embeddings:
            return False
    
    queries = []
    
    if test_type == 'bm25':
        for query_text in SEARCH_QUERIES:
            queries.append({
                "query_text": query_text,
                "search_type": "bm25",
                "limit": limit,
                "graphql": generate_bm25_query(query_text, collections, limit)
            })
    
    elif test_type == 'hybrid_01':
        for query_text in SEARCH_QUERIES:
            queries.append({
                "query_text": query_text,
                "search_type": "hybrid_01",
                "limit": limit,
                "graphql": generate_hybrid_query(query_text, embeddings[query_text], 0.1, collections, limit)
            })
    
    elif test_type == 'hybrid_09':
        for query_text in SEARCH_QUERIES:
            queries.append({
                "query_text": query_text,
                "search_type": "hybrid_09",
                "limit": limit,
                "graphql": generate_hybrid_query(query_text, embeddings[query_text], 0.9, collections, limit)
            })
    
    elif test_type == 'vector':
        for query_text in SEARCH_QUERIES:
            queries.append({
                "query_text": query_text,
                "search_type": "vector",
                "limit": limit,
                "graphql": generate_vector_query(embeddings[query_text], collections, limit)
            })
    
    elif test_type == 'mixed':
        # Mix of all three types
        for i, query_text in enumerate(SEARCH_QUERIES):
            if i % 3 == 0:
                search_type = 'bm25'
                graphql = generate_bm25_query(query_text, collections, limit)
            elif i % 3 == 1:
                search_type = 'hybrid_01'
                graphql = generate_hybrid_query(query_text, embeddings[query_text], 0.1, collections, limit)
            else:
                search_type = 'hybrid_09'
                graphql = generate_hybrid_query(query_text, embeddings[query_text], 0.9, collections, limit)
            
            queries.append({
                "query_text": query_text,
                "search_type": search_type,
                "limit": limit,
                "graphql": graphql
            })
    
    # Save to queries subfolder
    queries_dir = os.path.join(output_dir, 'queries')
    os.makedirs(queries_dir, exist_ok=True)
    
    filename = os.path.join(queries_dir, f"queries_{test_type}_{limit}.json")
    with open(filename, 'w') as f:
        json.dump(queries, f, indent=2)
    
    print(f"‚úÖ Created: {filename} ({len(queries)} queries)")
    return True


def main():
    parser = argparse.ArgumentParser(description='Generate ALL PT queries')
    parser.add_argument('--type', choices=['multi', 'single'], default='multi',
                        help='multi=9 collections, single=SongLyrics only')
    parser.add_argument('--limits', nargs='+', type=int, default=[10, 50, 100, 150, 200],
                        help='Result limits to generate (default: 10 50 100 150 200)')
    parser.add_argument('--search-types', nargs='+', 
                        choices=['bm25', 'hybrid_01', 'hybrid_09', 'vector', 'mixed'],
                        default=['bm25', 'hybrid_01', 'hybrid_09', 'vector', 'mixed'],
                        help='Search types to generate')
    
    args = parser.parse_args()
    
    # Set collections based on type
    if args.type == 'multi':
        collections = MULTI_COLLECTIONS
        output_dir = 'multi_collection'
        print("Generating queries for MULTI-COLLECTION (9 collections)")
    else:
        collections = [config.WEAVIATE_CLASS_NAME]
        output_dir = 'single_collection'
        print(f"Generating queries for SINGLE-COLLECTION ({config.WEAVIATE_CLASS_NAME})")
    
    print(f"Limits: {args.limits}")
    print(f"Search types: {args.search_types}")
    print("="*70)
    
    # Generate for each combination
    total = len(args.limits) * len(args.search_types)
    completed = 0
    
    for search_type in args.search_types:
        for limit in args.limits:
            if generate_all_query_files(search_type, limit, collections, output_dir):
                completed += 1
            else:
                print(f"‚ùå Failed to generate {search_type} limit {limit}")
    
    print("\n" + "="*70)
    print(f"‚úÖ Generated {completed}/{total} query files")
    print("="*70)
    
    return 0 if completed == total else 1


if __name__ == "__main__":
    import sys
    sys.exit(main())

